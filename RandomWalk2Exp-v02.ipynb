{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7004232f-9334-443e-a369-9fc8a0e74df0",
   "metadata": {},
   "source": [
    "This workbook provides a template for estimating SD models in Vensim using Bayesflow package. The code is structured so that all key architectural choices are made at the top (including Vensim model features) and the code can run for any model after setting those initials. The running of the full code is also functionalized, so it is easy to run more complex analysis and save key outputs as desired."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d3b67c-b324-4b35-b116-6f1ef2dbf3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Defining key structural parameters of the analysis '''\n",
    "import numpy as np\n",
    "\n",
    "# Vensim model and parameters\n",
    "vensimInputs={\n",
    "    'venModelName': 'RandomWalk2', #name of the vensim model (should be .vmpx for dll, and .mdlx for script-based)\n",
    "    'venVOCFile' : 'RandomWalk2.voc',  #A .voc or .out file from whichc estimated parameter names and ranges can be read (must include upper and lower bounds); if not available, leave as '' and populate 'venParameters' below\n",
    "    'SeedVar' : 'NSeed', # Name of the variable in vensim that controls the noise seed; default is 'NSeed'\n",
    "    'venParameters': {\"Drift\": (-1, 1), \"InitS\": (0, 10), \"NoiseStd\": (0, 0.5), \"MeasStd\" : (0, 0.5)},\n",
    "    'shortHandNames': [r\"$Drift$\", r\"$S0$\", r\"$\\sigma_P$\", r\"$\\sigma_M$\"], #Shorthand for parameter names to be used in graphs; leave empty, [], so it is populated based on actual vensim names\n",
    "    'outputs': ['State Obs'],  #vensim output variables to be recorded and trained on\n",
    "    'out_labels' : ['State Obs'], # labeles for output variables\n",
    "    'out_units' : ['Dimensionless'], # units for output variables\n",
    "    'constants' : ['Final Time'], #Name of constants to be set before start of the simulations\n",
    "    'constant_vals' : [99],  #Values of constants to be set for all simulations in a context\n",
    "    'Time_points' : np.arange(100), #All times for which data points should be extracted; should be consistent with Final Time- Start Time\n",
    "    'useDll' : True, # Using DLL (venpy) or script based (VST) connection with Vensim\n",
    "    'use_ven_multi' : True, #whether to use multicore or single core vensim\n",
    "    'vengine_path': \"C:/Program Files/Vensim/vendss64MC.exe\",  # Change this to your Vengine file path!\n",
    "    'vensim_path': \"C:/Program Files/Vensim/vendss64.exe\",  # Change this to your Vensim file path!\n",
    "}\n",
    "\n",
    "\n",
    "# Bayesflow inputs and parameters\n",
    "BFInputs={\n",
    "    #summary network settings\n",
    "    'sum_network_type' : 'sequence', # use sequence or transformer\n",
    "    'num_conv_layers' : 2, # Number of convolution layers for summary network\n",
    "    'lstm_units': 32, # number of lstm units in summary network\n",
    "    'bidirectional': False, #whether to use bidirectional or unidirectional in summary network\n",
    "    # transformer only settings\n",
    "    'num_heads' : 4, # number of heads for transformer\n",
    "    'key_dim' : 32, #key dimensionality for transformer\n",
    "    'num_attention_blocks' : 2, # dimensionality of attention blocks for transformer\n",
    "    'num_dense' : 2, # number of dense_fc for transformer\n",
    "    # inference network settings\n",
    "    'num_coupling_layers' : 2, # Number of coupling layers for inference network\n",
    "    # configurator settings\n",
    "    'float16' : False, #whether to use float16 or float32 for NN training; given the range of float16 it is a good idea to log-transform the results if using this option\n",
    "    'log_outputs': False, # whether to log-trasform the output data or use actual values\n",
    "    'normalize_params' : False, #whether to normalize parameters or use raw values\n",
    "    'remove_extremes' : True, #whether to remove any dataset with nan or inf values, or replace them with fixed values below\n",
    "    'nan_rplc' : -50000, # The value to replace nans in the simulations with; only active if not removing extremes\n",
    "    'posinf_rplc':  51000, # The value to replace positive infinity with; only active if not removing extremes\n",
    "    'neginf_rplc' : -51000, # The value to replace negative infinity with; only active if not removing extremes\n",
    "    # training settings\n",
    "    'learning_rate': 0.0005, # learning rate for training of networks\n",
    "    'TrainType' : 'rounds', # Use 'offline', 'online', or 'rounds', with default (any other text) leading to rounds\n",
    "    'num_rounds' : 5, # number of rounds for both offline and round-based training\n",
    "    'sims_per_round' : 1024, # number of sims per round; normalized for comparability\n",
    "    'epochs' : 20, # number of epochs per round, or in case of online training, total number of epochs (each with iterations of batchs)\n",
    "    'batch_size' : [32], # batch sizes for off(/on)line training rounds; for round based only the first size is used\n",
    "    # validation settings\n",
    "    'num_valid_sims' : 1000, # number of validation sims to use\n",
    "    # other settings\n",
    "    'modelName' : 'SEIRb', # Used for naming the model in Bayesflow, though plays little roll otherwise\n",
    "    'summary_dim' : 5, # Number of summary Dimensions\n",
    "    'trainer_memory' : True, # whether to hold a memory of simulations for later use\n",
    "    'save_checkpoint' : False, # whether to save the amortizer. Should modify the path below to point to the correct directory\n",
    "    'checkpoint_path': 'checkpoints/', # put a string for the folder to store trained amortizer, loss history, and optional memory\n",
    "    'reuse' : False, # whether the optimizer state should be kept for reusing; reuse may lead to very slow learning\n",
    "    # manual summary stat settings\n",
    "    'manual_sum_stats' : False, #whether to use manual summary stats for residual covariance\n",
    "    'window_sum_stats' : 11, # time window for savitzky-golay filter\n",
    "    'poly_sum_stats' : 2, #polynomial order for the filter\n",
    "    'lags_sum_stats' : [0,3,10], # lags used for calculating covariances\n",
    "}\n",
    "\n",
    "resInputs={\n",
    "    'graph_all': True\n",
    "}\n",
    "resInputs={\n",
    "    'scnr_nm' : 'RW_Overkill', #scenaior name used for saving graphs etc\n",
    "    'save_graphs' : True, #whether to save the graphs as jpeg\n",
    "    'valid_size': 500, #size of the validation dataset after training\n",
    "    'sample_size': 1000, #size of posterior samples for each of validation data\n",
    "    'rcvr_prcnl' : 80, # percentiles for recovery plots\n",
    "    'graph_prior' : resInputs['graph_all'], #whether to graph priors\n",
    "    'graph_losses' : resInputs['graph_all'], #whether to graph loss history\n",
    "    'graph_latent' : resInputs['graph_all'], #whether to graph latesnt space after training\n",
    "    'graph_SBC' : resInputs['graph_all'], #graph classical SBC using training data\n",
    "    'graph_ecdf' : resInputs['graph_all'], #whether to graph SBC ECDF charts for new validation SBC\n",
    "    'graph_recovery' : resInputs['graph_all'], #graph credible interval recovery\n",
    "    'graph_z_contract' : resInputs['graph_all'], #graph z-scores and contraction for outcomes\n",
    "    'graph_CI' : resInputs['graph_all'], #graph CI recovery quality\n",
    "    'posterior_predictive' : resInputs['graph_all'], #resInputs['graph_all'], #graph posteriors based on a single dataset\n",
    "    'ppsize' : 1, # number of single sample posteriors to draw and demonstrate\n",
    "}\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36c0f9c-a430-49de-a945-4f95b484b4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import various needed libraries \n",
    "#criticals, used in all cases\n",
    "\n",
    "import bayesflow as bf\n",
    "import datetime\n",
    "from functools import partial\n",
    "import pandas as pd\n",
    "import bayesflow.diagnostics as diag\n",
    "from bayesflow.amortizers import AmortizedPosterior\n",
    "from bayesflow.networks import InvertibleNetwork, SequenceNetwork, TimeSeriesTransformer\n",
    "from bayesflow.simulation import GenerativeModel, Prior, Simulator\n",
    "from bayesflow.trainers import Trainer\n",
    "from scipy import stats\n",
    "from scipy.stats import nbinom\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import shutil\n",
    "from ctypes import c_float\n",
    "import venpy\n",
    "from vst import *\n",
    "from scipy.signal import savgol_filter\n",
    "import seaborn as sns\n",
    "from sklearn.neighbors import KernelDensity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f90ed9-0485-4f8b-8187-a1002f340748",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to parse out the vensim parameters from .voc or .out files\n",
    "def parse_voc_file(filepath):\n",
    "    \"\"\"\n",
    "    Parses a .voc file and extracts parameters and their ranges, with correct splitting.\n",
    "    Args:\n",
    "    - filepath (str): Path to the .voc file.\n",
    "    Returns:\n",
    "    - dict: Dictionary with parameter names as keys and their ranges as values.\n",
    "    \"\"\"\n",
    "    variables = {}\n",
    "    with open(filepath, 'r') as file:\n",
    "        start_processing = False\n",
    "        for line in file:\n",
    "            if line.startswith(':'):\n",
    "                start_processing = True\n",
    "                continue\n",
    "\n",
    "            if start_processing and line.strip():\n",
    "                # Extract parts of the line\n",
    "                parts = line.strip().split('<=')\n",
    "                param = parts[1].strip()  # The parameter name is the second part\n",
    "                lower_bound = float(parts[0].strip())\n",
    "                upper_bound = float(parts[2].strip())\n",
    "                variables[param] = (lower_bound, upper_bound)\n",
    "\n",
    "    return variables\n",
    "    \n",
    "\n",
    "# define a prior for the generative model.\n",
    "def model_prior(variables):\n",
    "    \"\"\"Generates a random draw from the joint prior.\n",
    "    Args: \n",
    "    - parameter names and ranges\n",
    "    Returns: \n",
    "    - draws from the priors\n",
    "    \"\"\"\n",
    "    \n",
    "    # Generate random numbers and create a NumPy array\n",
    "    random_values = np.array([\n",
    "        np.random.uniform(low, high) for name, (low, high) in variables.items()\n",
    "    ])\n",
    "    return random_values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a942cad3-b665-46b4-b633-6dfc0c85e0f8",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# function for getting variables from bayesflow and creating the vensim sensitivity file required\n",
    "def create_tab_delimited_file(\n",
    "    parameter_matrix, parameter_names, filename, seed_start, SeedVar='NSeed'\n",
    "):\n",
    "    \"\"\"\n",
    "    Creates a tab-delimited text file from a NumPy array of parameters, including\n",
    "    NSeed and additional constant columns.\n",
    "\n",
    "    Args:\n",
    "        parameter_matrix (numpy.ndarray): A 2D array of parameters, with shape (#simulations, #parameters).\n",
    "        parameter_names (list): A list of parameter names, corresponding to the columns of the array.\n",
    "        constant_names (list): A list of names for the constant columns.\n",
    "        constant_values (list or numpy.ndarray): A list or array of constant values, with length equal to the number of constant names.\n",
    "        filename (str): The name of the text file to create.\n",
    "    \"\"\"\n",
    "\n",
    "    with open(filename, \"w\") as f:\n",
    "        # Write the header row with all column names\n",
    "        header = [SeedVar] + parameter_names\n",
    "        f.write(\"\\t\".join(header) + \"\\n\")\n",
    "\n",
    "        # Write the parameter values, NSeed, and constants for each simulation\n",
    "        for i in range(len(parameter_matrix)):\n",
    "            simulation_values = np.insert(parameter_matrix[i], 0, i + 1 + seed_start)  # Insert NSeed at the beginning\n",
    "            f.write(\"\\t\".join(map(str, simulation_values)) + \"\\n\")\n",
    "\n",
    "\n",
    "#This helper furnction will be used for creating the vensim input files (lst, vsc, etc)\n",
    "def write_strings_to_file(strings, filename):\n",
    "    \"\"\"\n",
    "    Writes a set of strings to a text file, each on a new line.\n",
    "\n",
    "    Args:\n",
    "        strings: A set of strings to write.\n",
    "        filename: The name of the text file to write to.\n",
    "    \"\"\"\n",
    "\n",
    "    with open(filename, \"w\") as f:\n",
    "        for string in strings:\n",
    "            f.write(f\"{string}\\n\")  # Write each string with a newline character\n",
    "\n",
    "def extract_sensitivities_sorted(model, variable_names, all_times, sensitivity_count, SeedVar='NSeed'):\n",
    "    \"\"\"\n",
    "    Extracts and sorts sensitivity data for given variable names and times based on 'NSeed' value.\n",
    "    \n",
    "    Args:\n",
    "    - model: The model object with the 'getsensitivity' method.\n",
    "    - variable_names (list of str): The names of the variables for which to extract data.\n",
    "    - all_times (list): List of all time points in the sensitivity simulation.\n",
    "    - sensitivity_count (int): The number of sensitivity simulations conducted.\n",
    "    \n",
    "    Returns:\n",
    "    - numpy.ndarray: Array of dimensions (#Sensitivity, #Times, #Variables), sorted by 'NSeed'.\n",
    "    \"\"\"\n",
    "    # Extract the NSeed values for sorting\n",
    "    vec, length = model.getsensitivity(SeedVar, 0)\n",
    "    nseed_values = np.ctypeslib.as_array(vec, shape=(length,))\n",
    "    \n",
    "    # Check that the number of NSeed values matches the expected sensitivity count\n",
    "    if length != sensitivity_count:\n",
    "        raise ValueError(f\"Expected sensitivity count {sensitivity_count}, but got {length}\")\n",
    "    \n",
    "    # Sort the indices of simulations based on NSeed values\n",
    "    sorted_indices = np.argsort(nseed_values)\n",
    "\n",
    "    # Initialize an array to hold the sorted sensitivities\n",
    "    sorted_sensitivities = np.empty((sensitivity_count, len(all_times), len(variable_names)))\n",
    "\n",
    "    # Extract and sort data for each variable and time point\n",
    "    for i, var_name in enumerate(variable_names):\n",
    "        for j, time in enumerate(all_times):\n",
    "            # Get the sensitivity vector for the current variable at the current time\n",
    "            vec, _ = model.getsensitivity(var_name, time)\n",
    "            sensitivities = np.ctypeslib.as_array(vec, shape=(sensitivity_count,))\n",
    "\n",
    "            # Sort the sensitivities based on sorted_indices\n",
    "            sorted_sensitivities[:, j, i] = sensitivities[sorted_indices]\n",
    "\n",
    "    return sorted_sensitivities\n",
    "\n",
    "\n",
    "\n",
    "# helper function to get VST to extract data from tab file of sensitivity runs, after sorting based on NSeed\n",
    "def extract_simulation_output(filepath, output_variables, SeedVar='NSeed'):\n",
    "    # Read the tab-delimited file\n",
    "    df = pd.read_csv(filepath, delimiter='\\t')\n",
    "\n",
    "    # Separate the parameter rows (where 'Time' is NaN) and output rows (where 'Time' is not NaN)\n",
    "    df_params = df[df['Time'].isna()]\n",
    "    df_output = df[df['Time'].notna()]\n",
    "    # Sort the parameter dataframe by 'NSeed' and get the sorted simulation IDs\n",
    "    df_params_sorted = df_params.sort_values(by=SeedVar)\n",
    "    sorted_simulations = df_params_sorted['Simulation'].values\n",
    "\n",
    "    # Get unique time points\n",
    "    times = df_output['Time'].unique()\n",
    "\n",
    "    # Initialize an array to hold the extracted and sorted data\n",
    "    sorted_extracted_data = np.empty((len(sorted_simulations), len(times), len(output_variables)))\n",
    "\n",
    "    # Extract and sort data for each simulation and output variable\n",
    "    for i, sim in enumerate(sorted_simulations):\n",
    "        sim_df = df_output[df_output['Simulation'] == sim]\n",
    "        for j, var in enumerate(output_variables):\n",
    "            sorted_extracted_data[i, :, j] = sim_df[var].values\n",
    "\n",
    "    return sorted_extracted_data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# calculating manual summary stats and appending to data\n",
    "\n",
    "def calculate_residuals_covariance(data, window_length, polyorder, lags):\n",
    "    n_batch, n_points, n_series = data.shape\n",
    "    all_results = []\n",
    "    \n",
    "    # Determine the size for #batch_series_covariance based on lags and n_series\n",
    "    max_cov_size = len(lags) * n_series + 1  # +1 for length at the end\n",
    "\n",
    "    for batch_index in range(n_batch):\n",
    "        current_batch = data[batch_index, :, :]\n",
    "        smoothed_batch = np.array([savgol_filter(current_batch[:, series], window_length, polyorder) \n",
    "                                   for series in range(n_series)]).T\n",
    "        residuals = current_batch - smoothed_batch\n",
    "        residuals[np.isnan(residuals)] = 0  # Replace NaNs with 0 for covariance calculation\n",
    "        \n",
    "        # Initialize an array for covariances, considering the adjusted output dimension\n",
    "        batch_covariances = np.zeros((max_cov_size, n_series))\n",
    "        \n",
    "        for i in range(n_series):\n",
    "            covariances = []\n",
    "            for j in range(n_series):  # Compute for all pairs\n",
    "                for lag in lags:\n",
    "                    if lag==0:\n",
    "                        series_i, series_j = residuals[:, i], residuals[:, j]\n",
    "                    else:\n",
    "                        series_i, series_j = residuals[:-lag, i], residuals[lag:, j]\n",
    "\n",
    "                    min_length = min(len(series_i), len(series_j))\n",
    "                    series_i, series_j = series_i[:min_length], series_j[:min_length]\n",
    "                    \n",
    "                    if len(series_i) > 0 and len(series_j) > 0:\n",
    "                        cov_ij = np.cov(series_i, series_j, ddof=0)[0, 1]\n",
    "                        covariances.append(cov_ij)\n",
    "                        \n",
    "            # transform to sqrt\n",
    "            covariances_sqr = np.sqrt(np.abs(covariances)) * np.sign(covariances)\n",
    "            \n",
    "            # Store the covariances and their length in the array\n",
    "            covariances_length = len(covariances_sqr)\n",
    "            batch_covariances[:covariances_length, i] = covariances_sqr\n",
    "            batch_covariances[covariances_length, i] = covariances_length  # Store length at the end\n",
    "        \n",
    "        all_results.append(batch_covariances)\n",
    "    \n",
    "    \n",
    "    return np.array(all_results)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d262f4-e3e0-4c13-a3da-fa8f95131298",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Simulation model in vensim, connected through dll or scripts. Note that this function is given the params (which vary across simulations) and various hyper parameters and variables.\n",
    "def vensimSimulationModel(params, venInputs, BFInputs):\n",
    "    \"\"\"Performs a batch of forward simulations from the SD model given a batch of random draws from the prior.\"\"\"\n",
    "    # First the setup of the function\n",
    "    venModelPub = venInputs['venModelName']+'.vpmx' #name of the published model\n",
    "    venModelNorm = venInputs['venModelName']+'.mdl' #name of the published model\n",
    "    simSubDir='VensimRuns'  #name of subdirectory to do simulations in when using scripts\n",
    "    venSensFile=\"SIRSens.txt\"  #name the sensitivity file vensim needs for reading parameters to simulate; will be created below.\n",
    "    venSaveList='sens.lst' #name of the savelist used in recording model outputs, to be created below\n",
    "    venSensControl = \"Sens.vsc\" #name of the sensitivity control file, to be created below\n",
    "    seed_start=np.random.randint(0, 10000000) #set a new start point for random numbers creating the noise streams\n",
    "    #Vensim parameter names (need exact text), order is important and should match those of the 'prior' parameters\n",
    "    ven_param_names = list(venInputs['venParameters'].keys())\n",
    "    #Vensim constants that are the same across all runs in a batch and match the inputs of the function\n",
    "    ven_const_names = venInputs['constants']\n",
    "    #Values of vensim constants\n",
    "    Constants = venInputs['constant_vals']\n",
    "    # Simulation outputs used in the analysis\n",
    "    output_vars = venInputs['outputs']  # output variable names in the order expected\n",
    "    # Number of data points to be extracted\n",
    "    Times = venInputs['Time_points'] \n",
    "    # Noise seed variable name\n",
    "    venSeedVar=venInputs['SeedVar']\n",
    "\n",
    " \n",
    "    # create vensim savelist for sensitivity\n",
    "    write_strings_to_file(output_vars, venSaveList)\n",
    "\n",
    "    # create sensitivity control file for vensim\n",
    "    write_strings_to_file(['200,F,1234,' + os.getcwd() + '/' + venSensFile + ',0'], venSensControl)\n",
    "\n",
    "    \n",
    "    # create the sensitivity file for vensim. This could include other constant-across-sim parameter changes not passed in setval\n",
    "    create_tab_delimited_file(params,ven_param_names, venSensFile, seed_start, SeedVar=venSeedVar)\n",
    "\n",
    "    if venInputs['useDll']:\n",
    "        # Read the model\n",
    "        venmodel = venpy.load(venModelPub)\n",
    "        # set general parameters\n",
    "        i = 0\n",
    "        for varNm in ven_const_names:\n",
    "            venmodel[varNm] = Constants[i]\n",
    "            i = i+1\n",
    "        # do the simulations\n",
    "        venmodel.run(runname='sens',sensitivity=[venSensControl,venSaveList])\n",
    "        # get the data\n",
    "        sim_data = extract_sensitivities_sorted(venmodel, output_vars, Times , params.shape[0], SeedVar=venSeedVar)\n",
    "    else:\n",
    "        # Removes the simulation subdirectory and its contents if it exists.\"\"\"\n",
    "        subdirectory_path = \"./\" + simSubDir\n",
    "        if os.path.exists(subdirectory_path) and os.path.isdir(subdirectory_path):\n",
    "            try:\n",
    "                # Use shutil.rmtree for robust deletion\n",
    "                shutil.rmtree(subdirectory_path)\n",
    "                print(f\"Subdirectory '{subdirectory_path}' and its contents deleted successfully.\")\n",
    "            except OSError as error:\n",
    "                print(f\"Error removing subdirectory '{subdirectory_path}': {error}\")\n",
    "    \n",
    "        \n",
    "        # creating tuple for setvals in the script for vensim. Note the wrapping of function inputs here.\n",
    "        setval_tuples = tuple(zip(ven_const_names, Constants))\n",
    "\n",
    "        # Global control inputs for vensim\n",
    "        timelimit = 300  # Time limit (in seconds) for automatic checking/monitoring of runs\n",
    "        if venInputs['use_ven_multi']:\n",
    "            vgpath = venInputs['vengine_path'] \n",
    "        else:\n",
    "            vgpath = venInputs['vensim_path'] \n",
    "        \n",
    "        # Setup the control file for the sensitivity run\n",
    "        # Controlfile inputs. The empty fields could be potentially removed, but kept here for generalizeability\n",
    "        cf = {\n",
    "            'basename': 'sens', #Note this name becomes the name of the sensitivity simulation\n",
    "            'simcontrol': {\n",
    "                \"model\": venModelNorm, #This is the name of the model in the root directory\n",
    "                \"data\": [], \n",
    "                \"payoff\": \"\", \n",
    "                \"sensitivity\": venSensControl, #This is the name of sensitivity control file; in it you should point to venSensFile in the correct directory\n",
    "                \"optparm\": \"\", \n",
    "                \"changes\": [], \n",
    "                \"savelist\": \"\",\n",
    "                \"setvals\": [],\n",
    "                \"senssavelist\": venSaveList #This savelist should include all the variables the model should give to bayesflow\n",
    "            }, \n",
    "            'runcmd': '', \n",
    "            'savecmd': 'SENS2FILE|!|!|T'    #This is the command file for exporting sensitivity data; you may need to change this if frequency of saving of data in vensim is different from what you need in bayesflow\n",
    "        }\n",
    "        \n",
    "        # Create logfile; not critical\n",
    "        log = f\"{os.getcwd()}/log.txt\"\n",
    "        \n",
    "        # Run the vensim command; this is where actual simulations happen\n",
    "        sens = Script(cf, \"\", log, setvals=setval_tuples, simtype='sf')\n",
    "        #sens.compile_script(vgpath, log, subdir=simSubDir, timelimit=timelimit, check_funcs=[check_output])\n",
    "        sens.compile_script(vgpath, log, subdir=simSubDir)\n",
    "        \n",
    "        # Take the outputs from tab and put into the desired format\n",
    "        \n",
    "        file_path = simSubDir + \"/\" + cf['basename'] + \".tab\" # file path for the results of simulations\n",
    "        \n",
    "        # Extract data\n",
    "        sim_data = extract_simulation_output(file_path, output_vars)\n",
    "    \n",
    "        # Remove helper vensim files to reduce clutter from baseline folder (but leaves intact the main simulation folder for inspection)\n",
    "        for filename in (venSensFile,venSaveList, venSensControl):\n",
    "            filepath = os.path.join(os.getcwd(), filename)  # Construct full path\n",
    "            try:\n",
    "                os.remove(filepath)  # Attempt to delete the file\n",
    "                print(f\"Deleted file: {filename}\")\n",
    "            except FileNotFoundError:\n",
    "                print(f\"File not found: {filename}\")\n",
    "            except OSError as error:\n",
    "                print(f\"Error deleting file {filename}: {error}\")\n",
    "\n",
    "    # calculating manual summary stats and inserting into time series format to bypass dimensionality restrictions\n",
    "    if BFInputs['manual_sum_stats']:\n",
    "        summary_stats=calculate_residuals_covariance(sim_data, BFInputs['window_sum_stats'], BFInputs['poly_sum_stats'], BFInputs['lags_sum_stats'])\n",
    "\n",
    "\n",
    "        # Determine new dimensions\n",
    "        n_batch, series_len, n_series = sim_data.shape\n",
    "        _, batch_series_covariance_plus_one, _ = summary_stats.shape\n",
    "        \n",
    "        # Initialize new_data with the correct shape\n",
    "        new_series_len = series_len + batch_series_covariance_plus_one\n",
    "        new_data = np.zeros((n_batch, new_series_len, n_series))\n",
    "        \n",
    "        # Copy original data into new_data\n",
    "        new_data[:, :series_len, :] = sim_data\n",
    "        \n",
    "        # Append residual_covariance at the end of each series\n",
    "        new_data[:, series_len:, :] = summary_stats\n",
    "        \n",
    "        # Update sim_data to the new array\n",
    "        sim_data = new_data\n",
    "\n",
    "    return sim_data\n",
    "\n",
    "\n",
    "\n",
    "# configurator helps log-transform, change format to float 32 or 16, and remove problematic sim values\n",
    "def configure_input(forward_dict, BFInputs):\n",
    "    \"\"\"Function to configure the simulated quantities (i.e., simulator outputs)\n",
    "    into a neural network-friendly (BayesFlow) format.\n",
    "    \"\"\"\n",
    "    # Prepare placeholder dict\n",
    "    out_dict = {}\n",
    "\n",
    "    if BFInputs['manual_sum_stats']:\n",
    "               \n",
    "        # Step 1: Determine length of summary stats from the last element in the second 'column'\n",
    "        x = int(forward_dict[\"sim_data\"][0, -1, 0])  # Assuming x is the same for all, convert to int due to float storage\n",
    "        \n",
    "        # Step 2: Extract summary_stats without using a for loop\n",
    "        summary_stats_all = forward_dict[\"sim_data\"][:, -x-1:, :]\n",
    "\n",
    "        # Step 3: falttening summary stats\n",
    "        summary_stats = summary_stats_all.reshape(summary_stats_all.shape[0], summary_stats_all.shape[1]*summary_stats_all.shape[2])\n",
    "        \n",
    "        # Step 4: add summary stats as direction conditions       \n",
    "        if BFInputs['float16']:\n",
    "            out_dict['direct_conditions'] = summary_stats.astype(np.float16)\n",
    "        else: \n",
    "            out_dict['direct_conditions'] = summary_stats.astype(np.float32)\n",
    "\n",
    "        showDiag=False\n",
    "        if showDiag:\n",
    "            data=forward_dict[\"sim_data\"]\n",
    "            n_batch, n_points, n_series = data.shape\n",
    "            plt.figure(figsize=(5,3))\n",
    "            for i in range(n_batch):\n",
    "                plt.plot(data[i,:,0], label=f'b {i+1}')\n",
    "    \n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "    \n",
    "            print(out_dict['direct_conditions'])\n",
    "\n",
    "\n",
    "\n",
    "    # Convert data to logscale\n",
    "    if BFInputs['float16']:\n",
    "        if BFInputs['log_outputs']:\n",
    "            outdata = np.log1p(forward_dict[\"sim_data\"]).astype(np.float16)\n",
    "        else:\n",
    "            outdata = forward_dict[\"sim_data\"].astype(np.float16)\n",
    "        \n",
    "        if BFInputs['normalize_params']:\n",
    "            # z-standardize with previously computed means\n",
    "            params = (forward_dict[\"prior_draws\"] - prior_means) / prior_stds\n",
    "        else: \n",
    "            params = forward_dict[\"prior_draws\"]\n",
    "        # turn to float 16\n",
    "        params = params.astype(np.float16)\n",
    "\n",
    "    else: \n",
    "        if BFInputs['log_outputs']:\n",
    "            outdata = np.log1p(forward_dict[\"sim_data\"]).astype(np.float32)\n",
    "        else:\n",
    "            outdata = forward_dict[\"sim_data\"].astype(np.float32)\n",
    "        \n",
    "        if BFInputs['normalize_params']:\n",
    "            # z-standardize with previously computed means\n",
    "            params = (forward_dict[\"prior_draws\"] - prior_means) / prior_stds\n",
    "        else: \n",
    "            params = forward_dict[\"prior_draws\"]\n",
    "            \n",
    "        # turn to float 32\n",
    "        params = params.astype(np.float32)\n",
    "\n",
    "    if BFInputs['remove_extremes']:\n",
    "        # Remove a batch if it contains nan, inf or -inf\n",
    "        idx_keep = np.all(np.isfinite(outdata), axis=(1, 2))\n",
    "        if not np.all(idx_keep):\n",
    "            print(\"Invalid value encountered...removing from batch\")\n",
    "        # Add to keys\n",
    "        out_dict[\"summary_conditions\"] = outdata[idx_keep]\n",
    "        out_dict[\"parameters\"] = params[idx_keep]\n",
    "    else:\n",
    "        outdataclean=np.nan_to_num(outdata, nan=BFInputs['nan_rplc'] , posinf=BFInputs['posinf_rplc'] , neginf= BFInputs['neginf_rplc'])\n",
    "        out_dict['summary_conditions'] = outdataclean\n",
    "        out_dict[\"parameters\"] = params\n",
    "\n",
    "\n",
    "\n",
    "    if BFInputs['sum_network_type']=='transformer':\n",
    "        x = out_dict['summary_conditions']\n",
    "        \n",
    "        # add time encoding to the data x\n",
    "        batch_size, num_timesteps, data_dim = x.shape\n",
    "        time_encoding = np.linspace(0, 1, num_timesteps)\n",
    "        time_encoding_batched = np.tile(time_encoding[np.newaxis, :, np.newaxis], (batch_size, 1, 1))\n",
    "        \n",
    "        out_dict['summary_conditions'] = np.concatenate((x, time_encoding_batched), axis=-1)\n",
    "\n",
    "    \n",
    "\n",
    "    return out_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0d957c-f2aa-4453-a2b3-4ec46bd40031",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating graphing functions\n",
    "# k-percentile ranges for conf intvls\n",
    "def centered_k_percentile_range(data, k, axis=None):\n",
    "    \"\"\"\n",
    "    Calculates the centered k-percentile range of a dataset.\n",
    "\n",
    "    Args:\n",
    "        data: A list or NumPy array of numerical data.\n",
    "        k: The percentile value (as a percentage, e.g., 25 for 25th percentile).\n",
    "        axis: The axis along which to calculate the range (default: None for a flattened array).\n",
    "\n",
    "    Returns:\n",
    "        The centered k-percentile range (the difference between the upper and lower k-percentiles).\n",
    "    \"\"\"\n",
    "    # Ensure data is a NumPy array for efficient operations\n",
    "    data = np.asarray(data)\n",
    "\n",
    "    lower_percentile = np.percentile(data, k, axis=axis)\n",
    "    upper_percentile = np.percentile(data, 100 - k, axis=axis)\n",
    "    return upper_percentile - lower_percentile\n",
    "\n",
    "# CI consistency graph\n",
    "def analyze_confidence_intervals_combined(input_sample, posterior_sample, param_names, N=20):\n",
    "    num_params = input_sample.shape[1]\n",
    "    num_columns = min(num_params, 6)\n",
    "    num_rows = int(np.ceil(num_params / num_columns))\n",
    "    \n",
    "    fig, axes = plt.subplots(num_rows, num_columns, figsize=(num_columns * 5, num_rows * 5))\n",
    "    if num_params > 1:\n",
    "        axes = axes.flatten()  # Flatten the axes array for easy indexing\n",
    "    else:\n",
    "        axes = [axes]  # Wrap in list if only one plot\n",
    "\n",
    "    for param_index in range(num_params):\n",
    "        ax = axes[param_index]\n",
    "        param_name = param_names[param_index]\n",
    "        ground_truths = input_sample[:, param_index]\n",
    "        posteriors = posterior_sample[:, :, param_index]\n",
    "\n",
    "        fractions = np.zeros(N)\n",
    "\n",
    "        for i in range(1, N + 1):\n",
    "            lower_percentile = 50 - 50 * i / N\n",
    "            upper_percentile = 50 + 50 * i / N\n",
    "\n",
    "            ci_lower = np.percentile(posteriors, lower_percentile, axis=1)\n",
    "            ci_upper = np.percentile(posteriors, upper_percentile, axis=1)\n",
    "\n",
    "            within_interval = (ground_truths >= ci_lower) & (ground_truths <= ci_upper)\n",
    "            fractions[i - 1] = np.mean(within_interval)\n",
    "\n",
    "        ax.plot(np.arange(1, N + 1) / N, fractions, marker='o', linestyle='-', color=\"#8f2727\", markersize=8, alpha=0.9)\n",
    "        ax.plot([0, 1], [0, 1], linestyle='dashed', color='black', linewidth=1.5, alpha=0.9)  # 45-degree reference line\n",
    "        ax.set_title(f\"{param_name}\", fontsize=21)\n",
    "        ax.set_xlabel(\"Interval Size\", fontsize=19)\n",
    "        ax.set_ylabel(\"Fraction Within Interval\", fontsize=19)\n",
    "        #ax.set_ylim(0, 1)\n",
    "        ax.set_ylim([-0.05, 1.05])\n",
    "        ax.tick_params(axis='both', which='major', labelsize=15)\n",
    "        ax.tick_params(axis='both', which='minor', labelsize=15)\n",
    "        ax.spines['top'].set_visible(False)\n",
    "        ax.spines['right'].set_visible(False)\n",
    "        ax.grid(alpha=0.5)\n",
    "        ax.grid(True)\n",
    "\n",
    "    for i in range(num_params, num_rows * num_columns):\n",
    "        fig.delaxes(axes[i])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    return fig\n",
    "\n",
    "\n",
    "def plot_posterior_2d_GT(\n",
    "    posterior_draws,\n",
    "    prior=None,\n",
    "    prior_draws=None,\n",
    "    param_names=None,\n",
    "    params_ground_truth=None,  # New parameter\n",
    "    height=3,\n",
    "    label_fontsize=14,\n",
    "    legend_fontsize=16,\n",
    "    tick_fontsize=12,\n",
    "    post_color=\"#8f2727\",\n",
    "    prior_color=\"gray\",\n",
    "    post_alpha=0.9,\n",
    "    prior_alpha=0.7,\n",
    "):\n",
    "    assert len(posterior_draws.shape) == 2, \"Shape of `posterior_samples` for a single data set should be 2 dimensional!\"\n",
    "    n_draws, n_params = posterior_draws.shape\n",
    "\n",
    "    if prior is not None and prior_draws is None:\n",
    "        draws = prior(n_draws)\n",
    "        prior_draws = draws[\"prior_draws\"] if type(draws) is dict else draws\n",
    "\n",
    "    if param_names is None:\n",
    "        param_names = [f\"$\\\\theta_{{{i}}}$\" for i in range(1, n_params + 1)]\n",
    "\n",
    "    posterior_draws_df = pd.DataFrame(posterior_draws, columns=param_names)\n",
    "\n",
    "    g = sns.PairGrid(posterior_draws_df, height=height)\n",
    "    g.map_diag(sns.histplot, fill=True, color=post_color, alpha=post_alpha, kde=True)\n",
    "    g.map_lower(sns.kdeplot, fill=True, color=post_color, alpha=post_alpha)\n",
    "\n",
    "    if prior_draws is not None:\n",
    "        prior_draws_df = pd.DataFrame(prior_draws, columns=param_names)\n",
    "        g.data = prior_draws_df\n",
    "        g.map_diag(sns.histplot, fill=True, color=prior_color, alpha=prior_alpha, kde=True, zorder=-1)\n",
    "        g.map_lower(sns.kdeplot, fill=True, color=prior_color, alpha=prior_alpha, zorder=-1)\n",
    "\n",
    "    if params_ground_truth is not None:\n",
    "        # Add ground truth as vertical lines and dots\n",
    "        for i, val in enumerate(params_ground_truth):\n",
    "            g.axes[i, i].axvline(val, color='k', linestyle='--')\n",
    "            for j in range(i):\n",
    "                g.axes[i, j].plot(params_ground_truth[j], val, 'ko')\n",
    "            for j in range(i+1, n_params):\n",
    "                g.axes[j, i].plot(val, params_ground_truth[j], 'ko')\n",
    "\n",
    "    if prior_draws is not None or prior is not None:\n",
    "        handles = [Line2D([], [], color=post_color, lw=3, alpha=post_alpha),\n",
    "                   Line2D([], [], color=prior_color, lw=3, alpha=prior_alpha)]\n",
    "        g.fig.legend(handles, [\"Posterior\", \"Prior\"], fontsize=legend_fontsize, loc=\"center right\")\n",
    "\n",
    "    for i, j in zip(*np.triu_indices_from(g.axes, 1)):\n",
    "        g.axes[i, j].axis(\"off\")\n",
    "\n",
    "    for i, j in zip(*np.tril_indices_from(g.axes, 1)):\n",
    "        g.axes[i, j].tick_params(axis=\"both\", which=\"major\", labelsize=tick_fontsize)\n",
    "        g.axes[i, j].tick_params(axis=\"both\", which=\"minor\", labelsize=tick_fontsize)\n",
    "\n",
    "    for i, param_name in enumerate(param_names):\n",
    "        g.axes[i, 0].set_ylabel(param_name, fontsize=label_fontsize)\n",
    "        g.axes[len(param_names) - 1, i].set_xlabel(param_name, fontsize=label_fontsize)\n",
    "\n",
    "    for i in range(n_params):\n",
    "        for j in range(n_params):\n",
    "            g.axes[i, j].grid(alpha=0.5)\n",
    "\n",
    "    g.tight_layout()\n",
    "    return g.fig\n",
    "\n",
    "\n",
    "def plot_ppc(real_data, post_sims, y_labels, titles, logscale=False, color=\"#8f2727\", figsize=(10, 6), font_size=14, cut_end=False):\n",
    "    \"\"\"\n",
    "    Plots posterior predictive checks for all series as subplots in a single figure.\n",
    "\n",
    "    Parameters:\n",
    "    - real_data: np.ndarray, shape [time_len, num_series]\n",
    "    - post_sims: np.ndarray, shape [num_batch, time_len, num_series]\n",
    "    - y_labels: list of str, Y axis labels for each series\n",
    "    - titles: list of str, titles for each subplot\n",
    "    - logscale: bool, if True, Y axis will be in logarithmic scale\n",
    "    - color: str, color for prediction intervals and median line\n",
    "    - figsize: tuple, overall figure size\n",
    "    - font_size: int, font size for text elements\n",
    "    \n",
    "    Returns:\n",
    "    - f: plt.Figure, the combined figure with all subplots\n",
    "    \"\"\"\n",
    "    plt.rcParams[\"font.size\"] = font_size\n",
    "    num_series = real_data.shape[1]\n",
    "    if cut_end:\n",
    "        x_len=int(post_sims.shape[1]-post_sims[0,-1,0]-1)\n",
    "    else:\n",
    "        x_len = post_sims.shape[1]\n",
    "    f, axes = plt.subplots(num_series, 1, figsize=figsize, constrained_layout=True)\n",
    "    \n",
    "    if num_series == 1:\n",
    "        axes = [axes]  # Ensure axes is iterable for a single subplot\n",
    "    \n",
    "    for i, ax in enumerate(axes):\n",
    "        sims = np.array(post_sims)[:,:x_len,i]\n",
    "        real_series_data = real_data[:x_len,i]\n",
    "        \n",
    "        qs_50 = np.quantile(sims, q=[0.25, 0.75], axis=0)\n",
    "        qs_90 = np.quantile(sims, q=[0.05, 0.95], axis=0)\n",
    "        qs_95 = np.quantile(sims, q=[0.025, 0.975], axis=0)\n",
    "        \n",
    "        ax.plot(np.median(sims, axis=0), label=\"Median simulated\", color=color)\n",
    "        ax.plot(real_series_data, marker=\"o\", label=\"Data\", color=\"black\", linestyle=\"dashed\", alpha=0.8)\n",
    "        ax.fill_between(range(real_series_data.shape[0]), qs_50[0], qs_50[1], color=color, alpha=0.5, label=\"50% CI\")\n",
    "        ax.fill_between(range(real_series_data.shape[0]), qs_90[0], qs_90[1], color=color, alpha=0.3, label=\"90% CI\")\n",
    "        ax.fill_between(range(real_series_data.shape[0]), qs_95[0], qs_95[1], color=color, alpha=0.1, label=\"95% CI\")\n",
    "        \n",
    "        ax.grid(color=\"grey\", linestyle=\"-\", linewidth=0.25, alpha=0.5)\n",
    "        ax.spines[\"right\"].set_visible(False)\n",
    "        ax.spines[\"top\"].set_visible(False)\n",
    "        ax.set_xlabel(\"Time\")\n",
    "        ax.set_ylabel(y_labels[i])\n",
    "        ax.set_title(titles[i])\n",
    "        if logscale:\n",
    "            ax.set_yscale(\"log\")\n",
    "        ax.legend(fontsize=font_size-2)\n",
    "        ax.minorticks_off()\n",
    "    f.show()\n",
    "        \n",
    "    return f\n",
    "\n",
    "def estimate_map(samples):\n",
    "    bw = 1.06 * samples.std() * samples.size ** (-1 / 5.)\n",
    "    scores = KernelDensity(bandwidth=bw).fit(samples.reshape(-1, 1)).score_samples(samples.reshape(-1, 1))\n",
    "    max_i = scores.argmax()\n",
    "    map_i = samples[max_i]\n",
    "    return map_i\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4177f40-f9f0-4fb4-8e6a-1d4c775653dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_full_analysis(vensimInputs, BFInputs, resInputs):\n",
    "\n",
    "    # get the input parameters for priors distribution\n",
    "    if vensimInputs['venVOCFile']:\n",
    "        venParams=parse_voc_file(vensimInputs['venVOCFile'])\n",
    "    else:\n",
    "        venParams= vensimInputs['venParameters']\n",
    "    #update vensimInputs\n",
    "    vensimInputs['venParameters']=venParams\n",
    "    \n",
    "    # get shorthands for parameter names\n",
    "    if vensimInputs['shortHandNames']:\n",
    "        venParNames=vensimInputs['shortHandNames']\n",
    "    else:\n",
    "        venParNames=list(venParams.keys())\n",
    "        \n",
    "    # build te prior object\n",
    "    prior = Prior(prior_fun=partial(model_prior, variables=venParams), param_names=venParNames)\n",
    "    # test if it works and get its basic stats\n",
    "    prior_means, prior_stds = prior.estimate_means_and_stds(1000)\n",
    "    \n",
    "    # We build the simulator using a partial function\n",
    "    simulator = Simulator(batch_simulator_fun=partial(vensimSimulationModel, venInputs=vensimInputs, BFInputs=BFInputs))\n",
    "    \n",
    "    # creating the full generative model by bringing together the simulator and prior\n",
    "    \n",
    "    model = GenerativeModel(prior, simulator, name=BFInputs['modelName'])\n",
    "    \n",
    "    # As per default, the plot_prior2d function will obtain 1000 draws from the joint prior.\n",
    "    if resInputs['graph_prior']:\n",
    "        f = prior.plot_prior2d(n_samples=100)\n",
    "        if resInputs['save_graphs']:\n",
    "            f.savefig(resInputs['scnr_nm']+'_prior.svg', format='svg',dpi=300)\n",
    "        \n",
    "\n",
    "    if BFInputs['sum_network_type']=='sequence':\n",
    "        # summary network, using a SequenceNetwork for time series\n",
    "        summary_net = SequenceNetwork(summary_dim=BFInputs['summary_dim'], num_conv_layers=BFInputs['num_conv_layers'], lstm_units=BFInputs['lstm_units'],\n",
    "                                      bidirectional=BFInputs['bidirectional'])\n",
    "    elif BFInputs['sum_network_type']=='transformer':\n",
    "        summary_net = TimeSeriesTransformer( len(vensimInputs['outputs']), attention_settings=dict(num_heads=BFInputs['num_heads'], key_dim=BFInputs['key_dim'], dropout=0.1), \n",
    "                                            dense_settings = None, use_layer_norm= True, num_dense_fc= BFInputs['num_dense'], summary_dim= BFInputs['summary_dim'], \n",
    "                                            num_attention_blocks=BFInputs['num_attention_blocks'], template_type='lstm', bidirectional=BFInputs['bidirectional'],\n",
    "                                            template_dim=BFInputs['lstm_units'])\n",
    "\n",
    "    \n",
    "    # inference network\n",
    "    inference_net = InvertibleNetwork(num_params=len(prior.param_names), num_coupling_layers=BFInputs['num_coupling_layers'])\n",
    "    \n",
    "    # amortizer definition glues together summary and inference networks\n",
    "    amortizer = AmortizedPosterior(inference_net, summary_net, name=BFInputs['modelName']+\"_amortizer\")\n",
    "\n",
    "    # fixing BFinputs for configurator\n",
    "    configure_input_fixed = partial(configure_input, BFInputs=BFInputs)\n",
    "    \n",
    "    # defining the trainer\n",
    "    trainer = Trainer(amortizer=amortizer, generative_model=model, configurator=configure_input_fixed, default_lr=BFInputs['learning_rate'], \n",
    "                      memory=BFInputs['trainer_memory'], checkpoint_path=BFInputs['checkpoint_path'])\n",
    "    \n",
    "    amortizer.summary()\n",
    "    #define an output variable to record all desired outputs and output after running the big function\n",
    "    runOutputs={}\n",
    "    runOutputs['param_count']=amortizer.count_params()\n",
    "    \n",
    "    # reset the random number generator so different runs are comparable more easily\n",
    "    RNG = np.random.default_rng(2023)\n",
    "    start_time=time.time()\n",
    "    # Main training block, choosing training mode and executing\n",
    "    if BFInputs['TrainType']=='offline':\n",
    "        for i in range(BFInputs['num_rounds']):\n",
    "            offline_data = model(BFInputs['sims_per_round'])\n",
    "            history = trainer.train_offline(offline_data, epochs=BFInputs['epochs'], batch_size=BFInputs['batch_size'][min(i,len(BFInputs['batch_size'])-1)], \n",
    "                                            save_checkpoint=BFInputs['save_checkpoint'], validation_sims=BFInputs['num_valid_sims'], reuse_optimizer=BFInputs['reuse'])\n",
    "    elif BFInputs['TrainType']=='online':\n",
    "            for i in range(BFInputs['num_rounds']):\n",
    "                history = trainer.train_online(epochs=BFInputs['epochs'], iterations_per_epoch=round(BFInputs['sims_per_round']/BFInputs['batch_size'][min(i,len(BFInputs['batch_size'])-1)]), \n",
    "                                               batch_size=BFInputs['batch_size'][i], save_checkpoint=BFInputs['save_checkpoint'], validation_sims=BFInputs['num_valid_sims'], reuse_optimizer=BFInputs['reuse'])\n",
    "    else: \n",
    "         history = trainer.train_rounds(BFInputs['num_rounds'], BFInputs['sims_per_round'], epochs=BFInputs['epochs'], \n",
    "                                       batch_size=BFInputs['batch_size'][0], save_checkpoint=BFInputs['save_checkpoint'], validation_sims=BFInputs['num_valid_sims'], reuse_optimizer=BFInputs['reuse'])\n",
    "    \n",
    "    end_time=time.time()\n",
    "    # record training time\n",
    "    runOutputs['train_time']=end_time-start_time\n",
    "    \n",
    "    # graph loss history\n",
    "    if resInputs['graph_losses']:\n",
    "        f = diag.plot_losses(history[\"train_losses\"], history[\"val_losses\"], moving_average=True)\n",
    "        if resInputs['save_graphs']:\n",
    "            f.savefig(resInputs['scnr_nm']+'_loss.svg', format='svg',dpi=300)\n",
    "    runOutputs['val_loss']= np.mean(history[\"val_losses\"][-10:])\n",
    "    runOutputs['train_loss']= np.mean(history[\"train_losses\"][-10:])\n",
    "    \n",
    "    # graph latent space\n",
    "    if resInputs['graph_latent']:\n",
    "        f = trainer.diagnose_latent2d()\n",
    "        if resInputs['save_graphs']:\n",
    "            f.savefig(resInputs['scnr_nm']+'_latent.svg', format='svg',dpi=300)\n",
    "    \n",
    "    # graph SBC basics\n",
    "    if resInputs['graph_SBC']:\n",
    "        f = trainer.diagnose_sbc_histograms()\n",
    "        if resInputs['save_graphs']:\n",
    "            f.savefig(resInputs['scnr_nm']+'_sbc.svg', format='svg',dpi=300)\n",
    "\n",
    "    for i in range(resInputs['ppsize']):\n",
    "        # Generate some validation data\n",
    "        validation_sims = trainer.configurator(model(batch_size=resInputs['valid_size']))\n",
    "        \n",
    "        # Generate posterior draws for all simulations\n",
    "        post_samples = amortizer.sample(validation_sims, n_samples=resInputs['sample_size'])\n",
    "        \n",
    "        #fsbc = diag.plot_sbc_histograms(post_samples, validation_sims[\"parameters\"], param_names=prior.param_names)\n",
    "        # Create ECDF plot\n",
    "        if resInputs['graph_ecdf']:\n",
    "            f = diag.plot_sbc_ecdf(post_samples, validation_sims[\"parameters\"], param_names=prior.param_names,difference=True)\n",
    "            if resInputs['save_graphs']:\n",
    "                f.savefig(resInputs['scnr_nm']+ '_' + str(i) +'_ecdf.svg', format='svg',dpi=300)\n",
    "        \n",
    "        # calculate required percentiles\n",
    "        k=(100-resInputs['rcvr_prcnl'])/2\n",
    "        \n",
    "        # graph recovery of parameters\n",
    "        if resInputs['graph_recovery']:\n",
    "            f = diag.plot_recovery(\n",
    "                post_samples,\n",
    "                validation_sims[\"parameters\"],\n",
    "                param_names=prior.param_names,\n",
    "                uncertainty_agg=lambda data, axis=None: centered_k_percentile_range(data, k, axis=axis)\n",
    "            )\n",
    "            if resInputs['save_graphs']:\n",
    "                f.savefig(resInputs['scnr_nm']+ '_' + str(i) +'_recovery.svg', format='svg',dpi=300)\n",
    "        \n",
    "        \n",
    "        # Calculate MAD for each sample (second dimension)\n",
    "        mad_per_sample = np.apply_along_axis(func1d=stats.median_abs_deviation, axis=1, arr=post_samples)\n",
    "        \n",
    "        # Average MAD across the first dimension\n",
    "        average_mad = np.mean(mad_per_sample, axis=0)\n",
    "        \n",
    "        # Format and print each element individually\n",
    "        formatted_average_mad = []\n",
    "        for value in average_mad:\n",
    "            formatted_average_mad.append(np.format_float_positional(value, precision=2, fractional=False))\n",
    "        \n",
    "        runOutputs['ave_MAD']=formatted_average_mad\n",
    "        \n",
    "        \n",
    "        # Calculate posterior mean and standard deviation\n",
    "        posterior_mean = np.mean(post_samples, axis=1)  # Mean across the samples (axis=1), results in (500, 4)\n",
    "        posterior_std = np.std(post_samples, axis=1)  # Std across the samples (axis=1), results in (500, 4)\n",
    "        \n",
    "        # Calculate the post_z_score and contraction\n",
    "        true_parameters = validation_sims['parameters']\n",
    "        post_z_score = (posterior_mean - true_parameters) / posterior_std  # Results in (500, 4)\n",
    "        post_contraction = 1-(np.var(post_samples, axis=1))/prior_stds**2\n",
    "        runOutputs['z_mean'] = np.mean(post_z_score, axis=0)\n",
    "        runOutputs['z_std'] = np.std(post_z_score, axis=0)\n",
    "        runOutputs['cnt_mean'] = np.mean(post_contraction, axis=0)\n",
    "        runOutputs['cnt_std'] = np.std(post_contraction, axis=0)\n",
    "        \n",
    "        # graph z scores and contractions\n",
    "        if resInputs['graph_z_contract']:\n",
    "            f= diag.plot_z_score_contraction(post_samples, validation_sims[\"parameters\"], param_names=prior.param_names)\n",
    "            if resInputs['save_graphs']:\n",
    "                f.savefig(resInputs['scnr_nm']+ '_' + str(i) +'_zcont.svg', format='svg',dpi=300)\n",
    "        \n",
    "        # graph consistency of confidence intervals generated by the estimation\n",
    "        if resInputs['graph_CI']:\n",
    "            f= analyze_confidence_intervals_combined(validation_sims[\"parameters\"], post_samples, prior.param_names)\n",
    "            if resInputs['save_graphs']:\n",
    "                f.savefig(resInputs['scnr_nm']+ '_' + str(i) +'_CI.svg', format='svg',dpi=300)\n",
    "\n",
    "    # graph posteriors for a single run\n",
    "    if resInputs['posterior_predictive']:\n",
    "        # Get lower and upper bounds for parameters\n",
    "        lower_bounds = np.array([bounds[0] for bounds in venParams.values()])\n",
    "        upper_bounds = np.array([bounds[1] for bounds in venParams.values()])\n",
    "        cut_end = BFInputs['manual_sum_stats']\n",
    "        for i in range(resInputs['ppsize']):\n",
    "            # Pick a single dataset as 'real' data\n",
    "            real_data = trainer.configurator(model(batch_size=1))\n",
    "\n",
    "            # Obtain a number of posterior draws given real data\n",
    "            post_samples_raw = amortizer.sample(real_data, n_samples=BFInputs['num_valid_sims'])\n",
    "            post_samples = np.clip(post_samples_raw, lower_bounds, upper_bounds)  # Ensure posterior samples are feasible\n",
    "\n",
    "            ground_truth = real_data['parameters'][0]\n",
    "\n",
    "            # Undo standardization to get parameters on their original (unstandardized) scales\n",
    "            if BFInputs['normalize_params']:\n",
    "                post_samples = prior_means + post_samples * prior_stds\n",
    "                ground_truth = prior_means + ground_truth * prior_stds\n",
    "\n",
    "            post_sims = simulator(post_samples)\n",
    "\n",
    "            # Generate summary statistics and save as CSV\n",
    "            qs_95 = np.quantile(post_samples, q=[0.025, 0.975], axis=0)\n",
    "            qs_95_str = ['[{0:.3f} - {1:.3f}]'.format(qs_95[0, j], qs_95[1, j]) for j in range(len(prior.param_names))]\n",
    "            meds = np.array(['{0:.3f}'.format(m) for m in np.median(post_samples, axis=0)])\n",
    "            means = np.array(['{0:.3f}'.format(m) for m in np.mean(post_samples, axis=0)])\n",
    "            maps = np.array(['{0:.3f}'.format(estimate_map(post_samples[:, j])) for j in range(len(prior.param_names))])\n",
    "\n",
    "            # Prepare table\n",
    "            table = pd.DataFrame(index=prior.param_names, data={\n",
    "                'Median': meds,\n",
    "                'Mean': means,\n",
    "                'MAP': maps,\n",
    "                '95-CI': qs_95_str\n",
    "            })\n",
    "\n",
    "            # Add ground truth parameters to the table\n",
    "            gt_params = ['{0:.3f}'.format(gt) for gt in ground_truth]\n",
    "            table['Ground Truth'] = gt_params\n",
    "\n",
    "            # Save table as CSV\n",
    "            table.to_csv(resInputs['scnr_nm'] + '_' + str(i) + '_posterior_summary.csv')\n",
    "\n",
    "            f1 = plot_ppc(\n",
    "                real_data['summary_conditions'][0, :, :],\n",
    "                post_sims['sim_data'],\n",
    "                y_labels=vensimInputs['out_units'],\n",
    "                titles=vensimInputs['out_labels'],\n",
    "                cut_end=cut_end\n",
    "            )\n",
    "            if resInputs['save_graphs']:\n",
    "                f1.savefig(resInputs['scnr_nm'] + '_' + str(i) + '_PPC.svg', format='svg', dpi=300)\n",
    "\n",
    "            f2 = plot_posterior_2d_GT(\n",
    "                post_samples,\n",
    "                param_names=prior.param_names,\n",
    "                params_ground_truth=ground_truth\n",
    "            )\n",
    "            if resInputs['save_graphs']:\n",
    "                f2.savefig(resInputs['scnr_nm'] + '_' + str(i) + '_posteriorEstimate.svg', format='svg', dpi=300)\n",
    "\n",
    "    print(\"Ground truth parameters:\", ground_truth)\n",
    "\n",
    "    return runOutputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ba5507-0293-486d-800b-f363d034732c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# function for running experiments and collecting data and also saving it\n",
    "def run_experiments_and_collect_data(base_vensimInputs, base_BFInputs, base_resInputs, experiments, save_path):\n",
    "    \"\"\"\n",
    "    Runs experiments with different conditions and collects outputs in a DataFrame, including full ndarrays.\n",
    "    \n",
    "    Parameters:\n",
    "    - base_vensimInputs: Dictionary with baseline vensim input values.\n",
    "    - base_BFInputs: Dictionary with baseline BF input values.\n",
    "    - base_resInputs: Dictionary with baseline res input values.\n",
    "    - experiments: List of dictionaries specifying the experimental conditions.\n",
    "    - save_path: Path to save the results DataFrame.\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame with the results of all experiments.\n",
    "    \"\"\"\n",
    "    all_results = []  # Initialize a list to store experiment results\n",
    "    \n",
    "    for exp in experiments:\n",
    "        # Copy baseline inputs\n",
    "        vensimInputs = base_vensimInputs.copy()\n",
    "        BFInputs = base_BFInputs.copy()\n",
    "        resInputs = base_resInputs.copy()\n",
    "        \n",
    "        # Update inputs based on experiment conditions\n",
    "        vensimInputs.update(exp.get('vensim_changes', {}))\n",
    "        BFInputs.update(exp.get('BF_changes', {}))\n",
    "        \n",
    "        # Run the simulation\n",
    "        outputs = run_full_analysis(vensimInputs, BFInputs, resInputs)\n",
    "        \n",
    "        # Prepare the result dictionary\n",
    "        result = {'experiment_label': exp['label']}\n",
    "        result.update(outputs)\n",
    "        \n",
    "        # Arrays will be stored directly in the DataFrame\n",
    "        all_results.append(result)\n",
    "    \n",
    "    # Convert the list of dictionaries to a DataFrame\n",
    "    results_df = pd.DataFrame(all_results)\n",
    "    \n",
    "    \n",
    "    directory = os.path.dirname(save_path)  # Extracts the directory part of the save_path\n",
    "    \n",
    "    # Check if the directory exists; if not, create it\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)  # This creates the directory and any intermediate directories\n",
    "        \n",
    "    # Save the DataFrame to disk for future retrieval\n",
    "    results_df.to_pickle(save_path)\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "\n",
    "\n",
    "# function for graphing the results of the experiment\n",
    "\n",
    "def plot_experiment_results(*results_dfs, x_key, y_key, x_label, y_label, df_labels, file_name, title_name):\n",
    "    \"\"\"\n",
    "    Plots the results of experiments as a scatter plot with lines connecting points within each DataFrame,\n",
    "    with custom axis labels and legend labels for each DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "    - *results_dfs: Variable number of DataFrame objects containing experiment results.\n",
    "    - x_key: The key for the column to be plotted on the X-axis.\n",
    "    - y_key: The key for the column to be plotted on the Y-axis.\n",
    "    - x_label: Label for the X-axis.\n",
    "    - y_label: Label for the Y-axis.\n",
    "    - df_labels: List of strings, labels for each DataFrame corresponding to results_dfs.\n",
    "    \"\"\"\n",
    "    if len(results_dfs) != len(df_labels):\n",
    "        raise ValueError(\"The number of DataFrames and df_labels must be the same.\")\n",
    "    \n",
    "    # Define markers and colors for differentiating between DataFrames\n",
    "    markers = ['o', 's', '^', 'D', 'x', '>', '<', 'p', '*', '+']  # Extended with more marker types\n",
    "    colors = ['b', 'g', 'r', 'c', 'm', 'y', 'k', 'orange', 'purple', 'brown']  # Extended with more colors\n",
    "\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    for df_index, df in enumerate(results_dfs):\n",
    "        if x_key not in df.columns or y_key not in df.columns:\n",
    "            print(f\"DataFrame {df_index} does not contain the specified keys.\")\n",
    "            continue\n",
    "        \n",
    "        x_values = df[x_key]\n",
    "        y_values = df[y_key]\n",
    "        labels = df['experiment_label']\n",
    "        \n",
    "        plt.scatter(x_values, y_values, label=df_labels[df_index], marker=markers[df_index % len(markers)], color=colors[df_index % len(colors)])\n",
    "        plt.plot(x_values, y_values, color=colors[df_index % len(colors)])  # Connect points with a line\n",
    "        \n",
    "        # Annotate points with labels\n",
    "        for (x, y, label) in zip(x_values, y_values, labels):\n",
    "            plt.text(x, y, label, fontsize=9)\n",
    "    \n",
    "    plt.xlabel(x_label)\n",
    "    plt.ylabel(y_label)\n",
    "    plt.legend()\n",
    "    plt.title(title_name)\n",
    "    plt.savefig(file_name + '.pdf', format='pdf',dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9382481a-0d77-495e-82ee-f1f47e7409f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the main part of code for defining the experiments, running them, and creating the desired graphs\n",
    "def update_flags(flags, indices, value):\n",
    "    \"\"\"\n",
    "    Update multiple flags at specific indices to either on (1) or off (0).\n",
    "    \n",
    "    Args:\n",
    "    - flags (list): The list of experiment flags.\n",
    "    - indices (list): The indices of the flags to update.\n",
    "    - value (int): The new value of the flags (1 for on, 0 for off).\n",
    "    \"\"\"\n",
    "    for index in indices:\n",
    "        if index < 0 or index >= len(flags):\n",
    "            print(f\"Index {index} out of range.\")\n",
    "            continue\n",
    "        flags[index] = value\n",
    "        \n",
    "exp_flags=[0]*15\n",
    "#update_flags(exp_flags,[0,1,3,4,5,6,13,14],1)\n",
    "update_flags(exp_flags,[],1)\n",
    "load_data=False\n",
    "\n",
    "'''\n",
    "experiments:\n",
    "experiment1_name='Batches'\n",
    "experiment2_name='lstm_units'\n",
    "experiment3_name='Batches_offline'\n",
    "experiment4_name='summary_dim'\n",
    "experiment5_name='conv_layer'\n",
    "experiment6_name='coupling_layer'\n",
    "experiment7_name='learning_rate'\n",
    "experiment8_name='float_bidir'\n",
    "experiment9_name='num_heads'\n",
    "experiment10_name='key_dim'\n",
    "experiment11_name='att_blocks'\n",
    "experiment12_name='dens_blocks'\n",
    "experiment13_name='bidirectional'\n",
    "experiment14_name='manual_summ'\n",
    "'''\n",
    "\n",
    "# update common parameter for offline experiment\n",
    "offBFInputs=BFInputs\n",
    "if exp_flags[0]:\n",
    "    # Define your baseline inputs and experimental conditions\n",
    "    experiment1_name='Batches'\n",
    "    experiments = [\n",
    "        {\n",
    "            'label': 'B16',\n",
    "            'vensim_changes': {},\n",
    "            'BF_changes': {'batch_size' : [16]}\n",
    "        },\n",
    "        {\n",
    "            'label': 'B32',\n",
    "            'vensim_changes': {},\n",
    "            'BF_changes': {'batch_size' : [32]}\n",
    "        },\n",
    "        {\n",
    "            'label': 'B64',\n",
    "            'vensim_changes': {},\n",
    "            'BF_changes': {'batch_size' : [64]}\n",
    "        },\n",
    "        {\n",
    "            'label': 'B128',\n",
    "            'vensim_changes': {},\n",
    "            'BF_changes': {'batch_size' : [128]}\n",
    "        },\n",
    "        {\n",
    "            'label': 'B256',\n",
    "            'vensim_changes': {},\n",
    "            'BF_changes': {'batch_size' : [256]}\n",
    "        },\n",
    "    ]\n",
    "    \n",
    "    # Define the path where you want to save the results\n",
    "    save_path = experiment1_name+'/results_dataframe' + experiment1_name + '.pkl'\n",
    "\n",
    "    if load_data:\n",
    "        results_df_1 = pd.read_pickle(save_path)\n",
    "    else: \n",
    "        # Run the experiments and save the results\n",
    "        results_df_1 = run_experiments_and_collect_data(vensimInputs, BFInputs, resInputs, experiments, save_path)\n",
    "\n",
    "\n",
    "# The results are now saved to 'results_dataframe.pkl' and can be loaded in the future\n",
    "# loaded_results_df = pd.read_pickle('path_to_save/results_dataframe.pkl')\n",
    "\n",
    "if exp_flags[1]:\n",
    "    experiment2_name='lstm_units'\n",
    "    experiments = [\n",
    "\n",
    "        {\n",
    "            'label': 'L64',\n",
    "            'vensim_changes': {},\n",
    "            'BF_changes': {'lstm_units': 64}\n",
    "        },\n",
    "        {\n",
    "            'label': 'L128',\n",
    "            'vensim_changes': {},\n",
    "            'BF_changes': {'lstm_units': 128}\n",
    "        },        \n",
    "        {\n",
    "            'label': 'L256',\n",
    "            'vensim_changes': {},\n",
    "            'BF_changes': {'lstm_units': 256}\n",
    "        },\n",
    "     \n",
    "    ]\n",
    "    \n",
    "    # Define the path where you want to save the results\n",
    "    save_path = experiment2_name+'/results_' + experiment2_name + '.pkl'\n",
    "    if load_data:\n",
    "        results_df_2 = pd.read_pickle(save_path)\n",
    "    else: \n",
    "        # Run the experiments and save the results\n",
    "        results_df_2 = run_experiments_and_collect_data(vensimInputs, BFInputs, resInputs, experiments, save_path)\n",
    "\n",
    "\n",
    "\n",
    "if exp_flags[2]:\n",
    "    # Define your baseline inputs and experimental conditions\n",
    "    experiment3_name='Batches_offline'\n",
    "    experiments = [\n",
    "        {\n",
    "            'label': 'OB16',\n",
    "            'vensim_changes': {},\n",
    "            'BF_changes': {'batch_size' : [16]}\n",
    "        },\n",
    "        {\n",
    "            'label': 'OB32',\n",
    "            'vensim_changes': {},\n",
    "            'BF_changes': {'batch_size' : [32]}\n",
    "        },\n",
    "        {\n",
    "            'label': 'OB64',\n",
    "            'vensim_changes': {},\n",
    "            'BF_changes': {'batch_size' : [64]}\n",
    "        },\n",
    "        {\n",
    "            'label': 'OB128',\n",
    "            'vensim_changes': {},\n",
    "            'BF_changes': {'batch_size' : [128]}\n",
    "        },\n",
    "        {\n",
    "            'label': 'OB256',\n",
    "            'vensim_changes': {},\n",
    "            'BF_changes': {'batch_size' : [256]}\n",
    "        },\n",
    "    ]\n",
    "    \n",
    "    # Define the path where you want to save the results\n",
    "    save_path = experiment3_name+'/results_dataframe' + experiment3_name + '.pkl'\n",
    "      \n",
    "    if load_data:\n",
    "        results_df_3 = pd.read_pickle(save_path)\n",
    "    else:\n",
    "        # Run the experiments and save the results\n",
    "        results_df_3 = run_experiments_and_collect_data(vensimInputs, offBFInputs, resInputs, experiments, save_path)\n",
    "\n",
    "if exp_flags[3]:\n",
    "    # Define your baseline inputs and experimental conditions\n",
    "    experiment4_name='summary_dim'\n",
    "    experiments = [\n",
    "        {\n",
    "            'label': 'Sd20',\n",
    "            'vensim_changes': {},\n",
    "            'BF_changes': {'summary_dim': 20}\n",
    "        },\n",
    "        {\n",
    "            'label': 'Sd30',\n",
    "            'vensim_changes': {},\n",
    "            'BF_changes': {'summary_dim': 30}\n",
    "        },\n",
    "        {\n",
    "            'label': 'Sd50',\n",
    "            'vensim_changes': {},\n",
    "            'BF_changes': {'summary_dim': 50}\n",
    "        },        \n",
    "    ]\n",
    "    \n",
    "    # Define the path where you want to save the results\n",
    "    save_path = experiment4_name+'/results_dataframe' + experiment4_name + '.pkl'\n",
    "    \n",
    "    if load_data:\n",
    "        results_df_4 = pd.read_pickle(save_path)\n",
    "    else:\n",
    "        # Run the experiments and save the results\n",
    "        results_df_4 = run_experiments_and_collect_data(vensimInputs, BFInputs, resInputs, experiments, save_path)\n",
    "    \n",
    "    \n",
    "    \n",
    "if exp_flags[4]:\n",
    "    \n",
    "    # Define your baseline inputs and experimental conditions\n",
    "    experiment5_name='conv_layer'\n",
    "    experiments = [\n",
    "        {\n",
    "            'label': 'Cl2',\n",
    "            'vensim_changes': {},\n",
    "            'BF_changes': {'num_conv_layers' : 2}\n",
    "        },\n",
    "        {\n",
    "            'label': 'Cl3',\n",
    "            'vensim_changes': {},\n",
    "            'BF_changes': {'num_conv_layers' : 3}\n",
    "        },\n",
    "        {\n",
    "            'label': 'Cl4',\n",
    "            'vensim_changes': {},\n",
    "            'BF_changes': {'num_conv_layers' : 4}\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Define the path where you want to save the results\n",
    "    save_path = experiment5_name+'/results_dataframe' + experiment5_name + '.pkl'\n",
    "    \n",
    "    if load_data:\n",
    "        results_df_5 = pd.read_pickle(save_path)\n",
    "    else:\n",
    "        # Run the experiments and save the results\n",
    "        results_df_5 = run_experiments_and_collect_data(vensimInputs, offBFInputs, resInputs, experiments, save_path)\n",
    "    \n",
    "if exp_flags[5]:   \n",
    "    # Define your baseline inputs and experimental conditions\n",
    "    experiment6_name='coupling_layer'\n",
    "    experiments = [\n",
    "        {\n",
    "            'label': 'Cp4',\n",
    "            'vensim_changes': {},\n",
    "            'BF_changes': {'num_coupling_layers' : 4}\n",
    "        },\n",
    "        {\n",
    "            'label': 'Cp6',\n",
    "            'vensim_changes': {},\n",
    "            'BF_changes': {'num_coupling_layers' : 6}\n",
    "        },\n",
    "        {\n",
    "            'label': 'Cp8',\n",
    "            'vensim_changes': {},\n",
    "            'BF_changes': {'num_coupling_layers' : 8}\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Define the path where you want to save the results\n",
    "    save_path = experiment6_name+'/results_dataframe' + experiment6_name + '.pkl'\n",
    "    \n",
    "    if load_data:\n",
    "        results_df_6 = pd.read_pickle(save_path)\n",
    "    else:\n",
    "        # Run the experiments and save the results\n",
    "        results_df_6 = run_experiments_and_collect_data(vensimInputs, offBFInputs, resInputs, experiments, save_path)\n",
    "    \n",
    "if exp_flags[6]:   \n",
    "    \n",
    "    # Define your baseline inputs and experimental conditions\n",
    "    experiment7_name='learning_rate'\n",
    "    experiments = [\n",
    "        {\n",
    "            'label': 'LR20',\n",
    "            'vensim_changes': {},\n",
    "            'BF_changes': {'learning_rate': 0.0020}\n",
    "        },\n",
    "        {\n",
    "            'label': 'LR10',\n",
    "            'vensim_changes': {},\n",
    "            'BF_changes': {'learning_rate': 0.0010}\n",
    "        },\n",
    "        {\n",
    "            'label': 'LR5',\n",
    "            'vensim_changes': {},\n",
    "            'BF_changes': {'learning_rate': 0.0005}\n",
    "        },\n",
    "    ]\n",
    "    \n",
    "    # Define the path where you want to save the results\n",
    "    save_path = experiment7_name+'/results_dataframe' + experiment7_name + '.pkl'\n",
    "    \n",
    "    if load_data:\n",
    "        results_df_7 = pd.read_pickle(save_path)\n",
    "    else:\n",
    "        # Run the experiments and save the results\n",
    "        results_df_7 = run_experiments_and_collect_data(vensimInputs, offBFInputs, resInputs, experiments, save_path)\n",
    "    \n",
    "if exp_flags[7]:    \n",
    "    \n",
    "    # Define your baseline inputs and experimental conditions\n",
    "    experiment8_name='float_bidir'\n",
    "    experiments = [\n",
    "        {\n",
    "            'label': 'f32b',\n",
    "            'vensim_changes': {},\n",
    "            'BF_changes': {'bidirectional': True}\n",
    "        },\n",
    "        {\n",
    "            'label': 'f16b',\n",
    "            'vensim_changes': {},\n",
    "            'BF_changes': {'float16' : True, 'bidirectional': True}\n",
    "        },\n",
    "        {\n",
    "            'label': 'f32u',\n",
    "            'vensim_changes': {},\n",
    "            'BF_changes': {'bidirectional': False}\n",
    "        },\n",
    "        {\n",
    "            'label': 'f16u',\n",
    "            'vensim_changes': {},\n",
    "            'BF_changes': {'float16' : True, 'bidirectional': False}\n",
    "        },\n",
    "    ]\n",
    "    \n",
    "    # Define the path where you want to save the results\n",
    "    save_path = experiment8_name+'/results_dataframe' + experiment8_name + '.pkl'\n",
    "    \n",
    "    if load_data:\n",
    "        results_df_8 = pd.read_pickle(save_path)\n",
    "    else:\n",
    "        # Run the experiments and save the results\n",
    "        results_df_8 = run_experiments_and_collect_data(vensimInputs, offBFInputs, resInputs, experiments, save_path)\n",
    "    \n",
    "if exp_flags[8]:    \n",
    "    \n",
    "    # Define your baseline inputs and experimental conditions\n",
    "    experiment9_name='num_heads'\n",
    "    experiments = [\n",
    "        {\n",
    "            'label': 'h2',\n",
    "            'vensim_changes': {},\n",
    "            'BF_changes': {'num_heads': 2}\n",
    "        },\n",
    "        {\n",
    "            'label': 'h4-base',\n",
    "            'vensim_changes': {},\n",
    "            'BF_changes': {'num_heads': 4}\n",
    "        },\n",
    "        {\n",
    "            'label': 'h8',\n",
    "            'vensim_changes': {},\n",
    "            'BF_changes': {'num_heads': 8}\n",
    "        },\n",
    "\n",
    "    ]\n",
    "    \n",
    "    # Define the path where you want to save the results\n",
    "    save_path = experiment9_name+'/results_dataframe' + experiment9_name + '.pkl'\n",
    "    \n",
    "    if load_data:\n",
    "        results_df_9 = pd.read_pickle(save_path)\n",
    "    else:\n",
    "        # Run the experiments and save the results\n",
    "        results_df_9 = run_experiments_and_collect_data(vensimInputs, BFInputs, resInputs, experiments, save_path)\n",
    "\n",
    "if exp_flags[9]:    \n",
    "    \n",
    "    # Define your baseline inputs and experimental conditions\n",
    "    experiment10_name='key_dim'\n",
    "    experiments = [\n",
    "        {\n",
    "            'label': 'k16',\n",
    "            'vensim_changes': {},\n",
    "            'BF_changes': {'key_dim': 16}\n",
    "        },\n",
    "        {\n",
    "            'label': 'k64',\n",
    "            'vensim_changes': {},\n",
    "            'BF_changes': {'key_dim': 64}\n",
    "        },\n",
    "\n",
    "    ]\n",
    "    \n",
    "    # Define the path where you want to save the results\n",
    "    save_path = experiment10_name+'/results_dataframe' + experiment10_name + '.pkl'\n",
    "    \n",
    "    if load_data:\n",
    "        results_df_10 = pd.read_pickle(save_path)\n",
    "    else:\n",
    "        # Run the experiments and save the results\n",
    "        results_df_10 = run_experiments_and_collect_data(vensimInputs, BFInputs, resInputs, experiments, save_path)\n",
    "\n",
    "if exp_flags[10]:    \n",
    "    \n",
    "    # Define your baseline inputs and experimental conditions\n",
    "    experiment11_name='att_blocks'\n",
    "    experiments = [\n",
    "        {\n",
    "            'label': 'ab3',\n",
    "            'vensim_changes': {},\n",
    "            'BF_changes': {'num_attention_blocks': 3}\n",
    "        },\n",
    "\n",
    "    ]\n",
    "    \n",
    "    # Define the path where you want to save the results\n",
    "    save_path = experiment11_name+'/results_dataframe' + experiment11_name + '.pkl'\n",
    "    \n",
    "    if load_data:\n",
    "        results_df_11 = pd.read_pickle(save_path)\n",
    "    else:\n",
    "        # Run the experiments and save the results\n",
    "        results_df_11 = run_experiments_and_collect_data(vensimInputs, BFInputs, resInputs, experiments, save_path)\n",
    "\n",
    "if exp_flags[11]:    \n",
    "    \n",
    "    # Define your baseline inputs and experimental conditions\n",
    "    experiment12_name='dens_blocks'\n",
    "    experiments = [\n",
    "        {\n",
    "            'label': 'db3',\n",
    "            'vensim_changes': {},\n",
    "            'BF_changes': {'num_dense': 3}\n",
    "        },\n",
    "\n",
    "    ]\n",
    "    \n",
    "    # Define the path where you want to save the results\n",
    "    save_path = experiment12_name+'/results_dataframe' + experiment12_name + '.pkl'\n",
    "    \n",
    "    if load_data:\n",
    "        results_df_12 = pd.read_pickle(save_path)\n",
    "    else:\n",
    "        # Run the experiments and save the results\n",
    "        results_df_12 = run_experiments_and_collect_data(vensimInputs, BFInputs, resInputs, experiments, save_path)\n",
    "\n",
    "if exp_flags[12]:    \n",
    "    \n",
    "    # Define your baseline inputs and experimental conditions\n",
    "    experiment13_name='bidirectional'\n",
    "    experiments = [\n",
    "        {\n",
    "            'label': 'uni_dir',\n",
    "            'vensim_changes': {},\n",
    "            'BF_changes': {'bidirectional': False}\n",
    "        },\n",
    "\n",
    "    ]\n",
    "    \n",
    "    # Define the path where you want to save the results\n",
    "    save_path = experiment13_name+'/results_dataframe' + experiment13_name + '.pkl'\n",
    "    \n",
    "    if load_data:\n",
    "        results_df_13 = pd.read_pickle(save_path)\n",
    "    else:\n",
    "        # Run the experiments and save the results\n",
    "        results_df_13 = run_experiments_and_collect_data(vensimInputs, BFInputs, resInputs, experiments, save_path)\n",
    "\n",
    "if exp_flags[13]:    \n",
    "    \n",
    "    # Define your baseline inputs and experimental conditions\n",
    "    experiment14_name='manual_summ'\n",
    "    experiments = [\n",
    "        {\n",
    "            'label': '+man',\n",
    "            'vensim_changes': {},\n",
    "            'BF_changes': {'manual_sum_stats': True}\n",
    "        },\n",
    "        {\n",
    "            'label': 'no_man',\n",
    "            'vensim_changes': {},\n",
    "            'BF_changes': {'manual_sum_stats': False}\n",
    "        },\n",
    "\n",
    "    ]\n",
    "    \n",
    "    # Define the path where you want to save the results\n",
    "    save_path = experiment14_name+'/results_dataframe' + experiment14_name + '.pkl'\n",
    "    \n",
    "    if load_data:\n",
    "        results_df_14 = pd.read_pickle(save_path)\n",
    "    else:\n",
    "        # Run the experiments and save the results\n",
    "        results_df_14 = run_experiments_and_collect_data(vensimInputs, BFInputs, resInputs, experiments, save_path)\n",
    "\n",
    "\n",
    "if exp_flags[14]:    \n",
    "    \n",
    "    # Define your baseline inputs and experimental conditions\n",
    "    experiment15_name='epochs'\n",
    "    experiments = [\n",
    "        {\n",
    "            'label': 'e10',\n",
    "            'vensim_changes': {},\n",
    "            'BF_changes': {'epochs': 10}\n",
    "        },\n",
    "        {\n",
    "            'label': 'e20',\n",
    "            'vensim_changes': {},\n",
    "            'BF_changes': {'epochs': 20}\n",
    "        },\n",
    "        {\n",
    "            'label': 'e30',\n",
    "            'vensim_changes': {},\n",
    "            'BF_changes': {'epochs': 30}\n",
    "        },\n",
    "    ]\n",
    "    \n",
    "    # Define the path where you want to save the results\n",
    "    save_path = experiment15_name+'/results_dataframe' + experiment15_name + '.pkl'\n",
    "    \n",
    "    if load_data:\n",
    "        results_df_15 = pd.read_pickle(save_path)\n",
    "    else:\n",
    "        # Run the experiments and save the results\n",
    "        results_df_15 = run_experiments_and_collect_data(vensimInputs, BFInputs, resInputs, experiments, save_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e66dbaf9-bc6a-4904-9991-687ce6857278",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Doing all the graphs\n",
    "\n",
    "'''\n",
    "experiments:\n",
    "experiment1_name='Batches'\n",
    "experiment2_name='lstm_units'\n",
    "experiment3_name='Batches_offline'\n",
    "experiment4_name='summary_dim'\n",
    "experiment5_name='conv_layer'\n",
    "experiment6_name='coupling_layer'\n",
    "experiment7_name='learning_rate'\n",
    "experiment8_name='float_bidir'\n",
    "experiment9_name='num_heads'\n",
    "experiment10_name='key_dim'\n",
    "experiment11_name='att_blocks'\n",
    "experiment12_name='dens_blocks'\n",
    "experiment13_name='bidirectional'\n",
    "experiment14_name='manual_summ'\n",
    "'''\n",
    "\n",
    "'''\n",
    "plot_experiment_results(results_df_1, results_df_2, results_df_3, results_df_4, x_key='train_time', y_key='val_loss', x_label='Training Time (seconds)',y_label='Loss Values',\n",
    "                       df_labels=['Batch Sizes','LSTM Units','Offline Batches','Summary Dimensions'],\n",
    "                        file_name = 'batch_LSTM_Summ_RandomWalk.pdf',title_name = 'Random Walk Hyper Parameters (Round Based)')\n",
    "\n",
    "\n",
    "plot_experiment_results(results_df_5, results_df_6, results_df_7, results_df_8, x_key='train_time', y_key='val_loss', x_label='Training Time (seconds)',y_label='Loss Values',\n",
    "                       df_labels=['Convolution Layers','Coupling Layers','Learning Rates','Float and bidirectional'],\n",
    "                        file_name = 'NN_Learning_float_RandomWalk',title_name = 'Random Walk Hyper Parameters (offline)')\n",
    "\n",
    "\n",
    "plot_experiment_results(results_df_2, results_df_9, results_df_10, results_df_11,results_df_12, results_df_13, x_key='train_time', y_key='val_loss', x_label='Training Time (seconds)',y_label='Loss Values',\n",
    "                       df_labels=['LSTM Units','Number of Heads','Key Dim','Attention Blocks','Dense Blocks','Bidirectional'],\n",
    "                        file_name = 'NN_Transformer_RandomWalk',title_name = 'Random Walk Transformer:500*7R*5E,32B')\n",
    "                      \n",
    "plot_experiment_results(results_df_14, x_key='train_time', y_key='val_loss', x_label='Training Time (seconds)',y_label='Loss Values',\n",
    "                       df_labels=['Manual Summary Stats'],\n",
    "                        file_name = 'NN_RandomWalk_manual',title_name = 'Random Walk Manual:500*5R*5E,32B')\n",
    "'''\n",
    "\n",
    "'''\n",
    "plot_experiment_results(results_df_1, x_key='train_time', y_key='val_loss', x_label='Training Time (seconds)',y_label='Loss Values',\n",
    "                       df_labels=['Batches'],\n",
    "                        file_name = 'Batches',title_name = 'Random Walk Hyper Parameters (Round Based)')\n",
    "\n",
    "plot_experiment_results(results_df_2, x_key='train_time', y_key='val_loss', x_label='Training Time (seconds)',y_label='Loss Values',\n",
    "                       df_labels=['LSTM Units'],\n",
    "                        file_name = 'LSTM Units',title_name = 'Random Walk Hyper Parameters (Round Based)')\n",
    "                        \n",
    "plot_experiment_results(results_df_4, x_key='train_time', y_key='val_loss', x_label='Training Time (seconds)',y_label='Loss Values',\n",
    "                       df_labels=['summary_dim'],\n",
    "                        file_name = 'Summary Dimensions',title_name = 'Random Walk Hyper Parameters (Round Based)')\n",
    "                        \n",
    "plot_experiment_results(results_df_5, x_key='train_time', y_key='val_loss', x_label='Training Time (seconds)',y_label='Loss Values',\n",
    "                       df_labels=['num_conv_layers'],\n",
    "                        file_name = 'Convolutional Layers',title_name = 'Random Walk Hyper Parameters (Round Based)')\n",
    "\n",
    "plot_experiment_results(results_df_6, x_key='train_time', y_key='val_loss', x_label='Training Time (seconds)',y_label='Loss Values',\n",
    "                       df_labels=['num_coupling_layers'],\n",
    "                        file_name = 'Coupling Layers',title_name = 'Random Walk Hyper Parameters (Round Based)')\n",
    "\n",
    "\n",
    "\n",
    "plot_experiment_results(results_df_7, x_key='train_time', y_key='val_loss', x_label='Training Time (seconds)',y_label='Loss Values',\n",
    "                       df_labels=['learning_rate'],\n",
    "                        file_name = 'Learning Rate',title_name = 'Random Walk Hyper Parameters (Round Based)')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plot_experiment_results(results_df_8, x_key='train_time', y_key='val_loss', x_label='Training Time (seconds)',y_label='Loss Values',\n",
    "                       df_labels=['float_bidir'],\n",
    "                        file_name = 'Float & Bidirectional',title_name = 'Random Walk Hyper Parameters (Round Based)')\n",
    "\n",
    "\n",
    "\n",
    "plot_experiment_results(results_df_14, x_key='train_time', y_key='val_loss', x_label='Training Time (seconds)',y_label='Loss Values',\n",
    "                       df_labels=['manual_sum_stats'],\n",
    "                        file_name = 'Manual Summary Statistics',title_name = 'Random Walk Hyper Parameters (Round Based)')\n",
    "\n",
    "'''                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa345e40-479f-414c-ac3d-e0b8ae185f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# an optimized run:\n",
    "\n",
    "runOptimized = True\n",
    "if runOptimized:\n",
    "    BFInputsOpt=BFInputs\n",
    "    #BFInputsOpt['epochs']=50\n",
    "    BFInputsOpt['epochs']=10\n",
    "    BFInputsOpt['batch_size']=[32]\n",
    "    BFInputsOpt['learning_rate']=0.0010\n",
    "    #BFInputsOpt['num_coupling_layers']= 6\n",
    "    BFInputsOpt['num_coupling_layers']= 6\n",
    "    BFInputsOpt['lstm_units']= 128\n",
    "    BFInputsOpt['summary_dim']= 20\n",
    "    BFInputsOpt['num_conv_layers']=4\n",
    "    BFInputsOpt['sum_network_type']='sequence'\n",
    "    BFInputsOpt['bidirectional']=True\n",
    "    BFInputsOpt['num_rounds']=20\n",
    "    #BFInputsOpt['sims_per_round']= 131072\n",
    "    BFInputsOpt['sims_per_round']= 8192\n",
    "    BFInputsOpt['manual_sum_stats'] = True\n",
    "\n",
    "    outputsOptTrans = run_full_analysis(vensimInputs, BFInputsOpt, resInputs)\n",
    "    \n",
    "    print(outputsOptTrans)\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722e318b-e2cd-4f8b-95a8-b59392ec9d6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdabf2df-0ad5-4143-bbf4-3596d6cdfd01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "815a5cb1-d282-4714-a0b6-d2ac618be863",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
